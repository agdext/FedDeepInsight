{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ykl24zvFPaIL"
   },
   "source": [
    "# FedDeepInsight SqueezeNet Example\n",
    "\n",
    "This example shows a case of using the output of\n",
    "pyDeepInsight.ImageTransformer to train a SqueezeNet model\n",
    "using PyTorch. This follows the general approach used in the\n",
    "original DeepInsight paper. As for the FL part we use methods that are similar to PyTorch notebook of image training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " !pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39Bs2NrxPaIO"
   },
   "source": [
    "## DeepInsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SACKo4eAOtcS",
    "outputId": "468d95b8-22b8-4798-b4b1-d6bdcf01ee1a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available. Using CPU.\")\n",
    "\n",
    "# Display the current device\n",
    "print(f\"Current device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHOW-rgTQotF",
    "outputId": "ba9cdc7a-c718-42a8-cb58-585875544b9b"
   },
   "outputs": [],
   "source": [
    "!python3 -m pip -q install git+https://github.com/alok-ai-lab/pyDeepInsight.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8Dr3UnEPaIP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyDeepInsight import ImageTransformer\n",
    "from pyDeepInsight.utils import Norm2Scaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXy1n0IQSxfb"
   },
   "source": [
    "View path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /gpfs/home3/rbumbuc1/FedLearning/FedDeepInsight/flower/TABULAR/FedDeepInsight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17beSelNPaIQ"
   },
   "outputs": [],
   "source": [
    "# Load your CSV data\n",
    "# data = pd.read_csv('/gpfs/home3/rbumbuc1/FedLearning/FedDeepInsight/datasets/stroke_balancedH.csv')\n",
    "data = pd.read_csv('/gpfs/home3/rbumbuc1/FedLearning/FedDeepInsight/datasets/cancer_test_headers.csv')\n",
    "# data = pd.read_csv('/gpfs/home3/rbumbuc1/FedLearning/FedDeepInsight/datasets/stroke_testnew.csv')\n",
    "\n",
    "\n",
    "# Assuming the first column is the target, separate features and target\n",
    "X = data.iloc[:, 1:].values #take values\n",
    "y = data.iloc[:, 0].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load your CSV data\n",
    "# data = pd.read_csv('/gpfs/home4/aduah/flower/dataset/stroke_2024.csv')\n",
    "# # data = pd.read_csv('/Users/allan/dataset/stroke_balancedH.csv')\n",
    "\n",
    "\n",
    "# # Assuming the first column is the target, separate features and target\n",
    "# X = data.iloc[:, 1:].values #take values\n",
    "# y = data.iloc[:, 0].values\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M8gFf3zgPaIQ",
    "outputId": "b4adefef-baba-4477-fa0b-454185b2eaf9"
   },
   "outputs": [],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQ5wyMpzPaIR"
   },
   "source": [
    "### Normalize data using LogScaler and encode classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOVJqMAFPaIS"
   },
   "outputs": [],
   "source": [
    "ln = Norm2Scaler()\n",
    "X_train_norm = ln.fit_transform(X_train)\n",
    "X_test_norm = ln.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6ka7ZOGPaIS"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "num_classes = np.unique(y_train_enc).size #IMPORTANT VAR!!----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1aRaaVrPaIS"
   },
   "source": [
    "### Create reducer object, change perplexity when using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGE perplexity THIS!!!!!!!!! 29 cancer/ 9 stroke\n",
    "perplexity= 29 #should be less than n_features\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rC_webE_PaIS"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "#t-SNE\n",
    "distance_metric = 'cosine'\n",
    "tsne_reducer = TSNE(\n",
    "    n_components=2,\n",
    "    metric=distance_metric,\n",
    "    init='random', \n",
    "    learning_rate='auto',\n",
    "    n_jobs=-1,\n",
    "    perplexity=perplexity, # should be less than n_features\n",
    "    random_state = random_state\n",
    ")\n",
    "#PCA\n",
    "pca_reducer =  PCA(n_components=2,     \n",
    "              random_state = random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import trustworthiness\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_trust(X_original, X_reduced, k=10):\n",
    "    \"\"\"\n",
    "    Computes Trustworthiness for dimensionality reduction.\n",
    "    \n",
    "    Parameters:\n",
    "        X_original (ndarray): Original high-dimensional data\n",
    "        X_reduced (ndarray): Reduced 2D data (PCA or t-SNE output)\n",
    "        k (int): Number of neighbors to consider\n",
    "\n",
    "    Returns:\n",
    "        trust (float), cont (float)\n",
    "    \"\"\"\n",
    "    trust = trustworthiness(X_original, X_reduced, n_neighbors=k)\n",
    "    \n",
    "    return trust\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trustworthiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Fit and transform\n",
    "# X_train_tsnered = tsne_reducer.fit_transform(X_train_norm)\n",
    "# X_train_pcared = pca_reducer.fit_transform(X_train_norm)\n",
    "\n",
    "# # Step 2: Evaluate structure preservation\n",
    "# trust = evaluate_trust(X_train_norm, X_train_tsnered, k=10)\n",
    "\n",
    "\n",
    "# # Step 3: Print results\n",
    "# print(f\"Trustworthiness : {trust:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# #for stroke\n",
    "# ks = [5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "\n",
    "# # #for cancer\n",
    "# # ks = [5, 10, 25, 50, 100, 200]\n",
    "# tsne_scores = [evaluate_trust(X_train_norm, X_train_tsnered, k=k) for k in ks]\n",
    "# pca_scores = [evaluate_trust(X_train_norm, X_train_pcared, k=k) for k in ks]\n",
    "\n",
    "# plt.plot(ks, tsne_scores, label='t-SNE', marker='o')\n",
    "# plt.plot(ks, pca_scores, label='PCA', marker='s')\n",
    "# plt.xlabel(\"k (Number of Neighbors)\")\n",
    "# plt.ylabel(\"Trustworthiness Score\")\n",
    "# plt.title(\"Trustworthiness vs. k\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import os\n",
    "\n",
    "# # Define the save path and filename\n",
    "# save_directory = \"/gpfs/home3/rbumbuc1/FedLearning/FedDeepInsight/flower/TABULAR/metrics\"\n",
    "# os.makedirs(save_directory, exist_ok=True)\n",
    "# csv_filename = \"trustworthiness_scores_cancer.csv\"\n",
    "# csv_filepath = os.path.join(save_directory, csv_filename)\n",
    "\n",
    "# # Write the data to the CSV file\n",
    "# with open(csv_filepath, mode='w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(['k', 't-SNE Score', 'PCA Score'])\n",
    "#     for k, tsne, pca in zip(ks, tsne_scores, pca_scores):\n",
    "#         writer.writerow([k, tsne, pca])\n",
    "\n",
    "# print(f\"Trustworthiness scores saved to: {csv_filepath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spearman & Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial.distance import pdist, squareform\n",
    "# from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "# def global_structure_score(X_high, X_low, method='spearman'):\n",
    "#     D_high = pdist(X_high, metric='euclidean')\n",
    "#     D_low = pdist(X_low, metric='euclidean')\n",
    "#     if method == 'spearman':\n",
    "#         score, _ = spearmanr(D_high, D_low)\n",
    "#     else:\n",
    "#         score, _ = pearsonr(D_high, D_low)\n",
    "#     return score\n",
    "\n",
    "# # Example usage\n",
    "# spearman_score_tsne = global_structure_score(X_train_norm, X_train_tsnered)\n",
    "# spearman_score_pca = global_structure_score(X_train_norm, X_train_pcared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(spearman_score_tsne,spearman_score_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_m_IDA-PaIT"
   },
   "source": [
    "### Initialize image transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZSoTBTlPaIT"
   },
   "outputs": [],
   "source": [
    "pixel_size = (227,227)\n",
    "it = ImageTransformer(\n",
    "    feature_extractor=pca_reducer,\n",
    "    pixels=pixel_size,\n",
    "    # discretization = 'lsa' \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYIPsh0PPaIT"
   },
   "source": [
    "Train image transformer on training data and transform training\n",
    "and testing sets. Values should be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C6HqyQehPaIT",
    "outputId": "80ab2692-4d27-48ab-ff02-6c57c6ba0b6a"
   },
   "outputs": [],
   "source": [
    "print(pca_reducer)\n",
    "print(X_train_norm)\n",
    "print(len(X_train_norm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "4SyxCsh1PaIT",
    "outputId": "ec20c46b-ed03-4728-adbc-be2f622d408a"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "it.fit(X_train_norm, y=y_train, plot=True)\n",
    "X_train_img = it.transform(X_train_norm)\n",
    "X_test_img = it.transform(X_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LO9QY5j-PaIT"
   },
   "source": [
    "The feature density matrix can be extracted from the trained transformer in order to view overall feature overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xmX6zmQPaIT"
   },
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "hUEoxcdSPaIU",
    "outputId": "118e376a-a6b6-46db-bb44-3064cb07c0d9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "fdm = it.feature_density_matrix()\n",
    "fdm[fdm == 0] = np.nan\n",
    "\n",
    "plt.figure(figsize=(10, 7.5))\n",
    "\n",
    "ax = sns.heatmap(fdm, cmap=\"viridis\", linewidths=0.,\n",
    "                 linecolor=\"lightgrey\", square=True)\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "for _, spine in ax.spines.items():\n",
    "    spine.set_visible(True)\n",
    "_ = plt.title(\"Feature per pixel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDQiWZuIPaIU"
   },
   "source": [
    "The following are showing plots for the image matrices first four samples\n",
    "of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "ZgndVxJCPaIU",
    "outputId": "15e15ddf-fde4-45cc-e055-425081f020e8"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i in range(0,3):\n",
    "    ax[i].imshow(X_train_img[i])\n",
    "    ax[i].title.set_text(f\"Train[{i}] - class '{y_train[i]}'\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35i5Mls7PaIU"
   },
   "source": [
    "Transforming the testing data is done the same as transforming the\n",
    "training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "km81VOcpPaIU",
    "outputId": "0e89e3eb-c338-4243-ba22-bdeb6eb632eb"
   },
   "outputs": [],
   "source": [
    "# X_test_img = it.transform(X_test_norm)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i in range(0,3):\n",
    "    ax[i].imshow(X_test_img[i])\n",
    "    ax[i].title.set_text(f\"Test[{i}] - class '{y_test[i]}'\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJGQghbnPaIU"
   },
   "source": [
    "## Generate Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wiXdhezPaIU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay\n",
    "\n",
    "import warnings;\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bye-6006PaIV"
   },
   "source": [
    "Transform numpy image format to PyTorch tensor. Using an untrained network,\n",
    "so normalization as specificed in SqueezeNet documentation is not\n",
    "required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dADhJGBPaIV"
   },
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVoghv1QPaIV"
   },
   "source": [
    "Generate pyTorch datasets and dataloaders for training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnEGRWOuPaIV"
   },
   "outputs": [],
   "source": [
    "X_train_tensor = torch.stack([preprocess(img) for img in X_train_img]).float()\n",
    "y_train_tensor = torch.from_numpy(le.fit_transform(y_train))\n",
    "\n",
    "X_test_tensor = torch.stack([preprocess(img) for img in X_test_img]).float()\n",
    "y_test_tensor = torch.from_numpy(le.transform(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eP1iyW7iPaIV"
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "\n",
    "trainset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLUMpMd29tHx"
   },
   "source": [
    "## CNN Architecure(Squeezenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zyw7O7KGHBG3"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.model = torchvision.models.squeezenet1_1(weights='DEFAULT')\n",
    "        self.model.classifier.append(nn.Flatten())\n",
    "        self.model.classifier.append(nn.Linear(1000, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMpiz9TiWjLE"
   },
   "outputs": [],
   "source": [
    "!pip install -q flwr[simulation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HB_NouPdFi14"
   },
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 20\n",
    "NUM_ROUNDS= 10\n",
    "EPOCHS = 10\n",
    "DATASET1 = 'Cancer dataset'\n",
    "DATASET2 = 'Stroke dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0GxylmmfHlo"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(num_partitions: int, batch_size: int, val_ratio: float = 0.1):\n",
    "    \"\"\"This function partitions the training set into N disjoint\n",
    "    subsets, each will become the local dataset of a client. This\n",
    "    function also subsequently partitions each traininset partition\n",
    "    into train and validation. The test set is left intact and will\n",
    "    be used by the central server to asses the performance of the\n",
    "    global model.\"\"\"\n",
    "\n",
    "    trainset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    testset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "\n",
    "    # split trainset into `num_partitions` trainsets (one per client)\n",
    "    # figure out number of training examples per partition\n",
    "    num_images = len(trainset) // num_partitions\n",
    "    remainder = len(trainset) % num_partitions\n",
    "\n",
    "    # a list of partition lenghts (all partitions are of equal size)\n",
    "    partition_len = [num_images] * num_partitions\n",
    "    for i in range(remainder):\n",
    "        partition_len[i] += 1\n",
    "\n",
    "    # split randomly. This returns a list of trainsets, each with `num_images` training examples\n",
    "    # Note this is the simplest way of splitting this dataset.\n",
    "    trainsets = random_split(\n",
    "        trainset, partition_len, torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    # create dataloaders with train+val support\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    # for each train set, let's put aside some training examples for validation\n",
    "    for trainset_ in trainsets:\n",
    "        num_total = len(trainset_)\n",
    "        num_val = int(val_ratio * num_total)\n",
    "        num_train = num_total - num_val\n",
    "\n",
    "        for_train, for_val = random_split(\n",
    "            trainset_, [num_train, num_val], torch.Generator().manual_seed(42)\n",
    "        )\n",
    "\n",
    "        # construct data loaders and append to their respective list.\n",
    "        # In this way, the i-th client will get the i-th element in the trainloaders list and the i-th element in the valloaders list\n",
    "        trainloaders.append(\n",
    "            DataLoader(for_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        )\n",
    "        valloaders.append(\n",
    "            DataLoader(for_val, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        )\n",
    "\n",
    "    # We leave the test set intact (i.e. we don't partition it)\n",
    "    # This test set will be left on the server side and we'll be used to evaluate the\n",
    "    # performance of the global model after each round.\n",
    "    # Please note that a more realistic setting would instead use a validation set on the server for\n",
    "    # this purpose and only use the testset after the final round.\n",
    "\n",
    "    testloader = DataLoader(testset, batch_size=batch_size)\n",
    "\n",
    "    return trainloaders, valloaders, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOrjMSR-hGIz"
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "\n",
    "trainloaders, valloaders, testloader = prepare_dataset(NUM_CLIENTS, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdFKJHLew115"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def train(net, trainloader, optimizer, epochs):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    net.train()\n",
    "    for _ in range(epochs):\n",
    "        # running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #     running_loss += loss.item()\n",
    "        # print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(trainloader):.4f}\")\n",
    "    return net\n",
    "\n",
    "# def test(net, testloader):\n",
    "#     \"\"\"Validate the network on the entire test set.\"\"\"\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "#     #correct, total, loss = 0, 0, 0.0\n",
    "#     correct, loss = 0, 0.0\n",
    "#     net.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in testloader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = net(images)\n",
    "#             loss += criterion(outputs, labels).item()\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             # total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "#     # accuracy = correct / total\n",
    "#     accuracy = correct / len(testloader.dataset)\n",
    "#     # avg_loss = loss / len(testloader)\n",
    "#     # print(f\"Accuracy on test set: {100 * accuracy:.2f}%\")\n",
    "#     # print(f\"Avg Loss on test set: {avg_loss:.4f}\")\n",
    "#     return loss, accuracy\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Validate the network on the entire test set with precision, recall, and F1.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    net.eval()\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / len(testloader.dataset)\n",
    "    precision = precision_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "\n",
    "    return loss, accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_centralised(epochs: int, lr: float, momentum: float = 0.9):\n",
    "    \"\"\"A minimal (but complete) training loop\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # uses gpu\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Instantiate the SqueezeNet model\n",
    "    model = Net(num_classes=num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    # Define criterion and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-04,\n",
    "    # momentum=0.8,\n",
    "    weight_decay=1e-05\n",
    ")\n",
    "\n",
    "    # Create datasets and data loaders\n",
    "    batch_size = 200\n",
    "\n",
    "    trainset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    testset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Train the model\n",
    "    trained_model = train(model, trainloader, optimizer, epochs)\n",
    "\n",
    "    # Test the model\n",
    "    loss, accuracy, precision, recall, f1 = test(trained_model, testloader)\n",
    "    \n",
    "    print(f\"{loss = }\")\n",
    "    print(f\"{accuracy = }\")\n",
    "    print(f\"{precision = }\")\n",
    "    print(f\"{recall = }\")\n",
    "    print(f\"{f1 = }\")\n",
    "\n",
    "\n",
    "    # Save metrics to CSV\n",
    "    save_dir = \"/gpfs/home3/rbumbuc1/FedLearning/FedDeepInsight/flower/TABULAR/metrics\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f\"Strokeorginal_Fmetrics_{epochs}E.csv\")\n",
    "\n",
    "    with open(save_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Loss', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "        writer.writerow([loss, accuracy, precision, recall, f1])\n",
    "\n",
    "    print(f\"Metrics saved to {save_path}\")\n",
    "\n",
    "    \n",
    "    # End the timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# Run the centralised training and testing\n",
    "# run_centralised(epochs=10, lr=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flower Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAFBz7r4DNnv"
   },
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "from collections import OrderedDict\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import torch\n",
    "from flwr.common import NDArrays, Scalar\n",
    "\n",
    "\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, trainloader, valloader) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.model = Net(num_classes)\n",
    "\n",
    "        # Determine device\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)  # send model to device\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        \"\"\"With the model parameters received from the server,\n",
    "        overwrite the uninitialise model in this class with them.\"\"\"\n",
    "\n",
    "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "        # now replace the parameters\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def get_parameters(self, config: Dict[str, Scalar]):\n",
    "        \"\"\"Extract all model parameters and convert them to a list of\n",
    "        NumPy arrays. The server doesn't work with PyTorch/TF/etc.\"\"\"\n",
    "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"This method train the model using the parameters sent by the\n",
    "        server on the dataset of this client. At then end, the parameters\n",
    "        of the locally trained model are communicated back to the server\"\"\"\n",
    "\n",
    "        # copy parameters sent by the server into client's local model\n",
    "        self.set_parameters(parameters)\n",
    "\n",
    "        # Define the optimizer -------------------------------------------------------------- Essentially the same as in the centralised example above\n",
    "        #optim = torch.optim.SGD(self.model.parameters(), lr=0.01, momentum=0.9)\n",
    "        optimizer = optim.AdamW(self.model.parameters(),lr=1e-04, weight_decay=1e-05)\n",
    "\n",
    "        # do local training  -------------------------------------------------------------- Essentially the same as in the centralised example above (but now using the client's data instead of the whole dataset)\n",
    "        train(self.model, self.trainloader, optimizer, epochs=EPOCHS)\n",
    "\n",
    "        # return the model parameters to the server as well as extra info (number of training examples in this case)\n",
    "        return self.get_parameters({}), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters: NDArrays, config: Dict[str, Scalar]):\n",
    "        \"\"\"Evaluate the model sent by the server on this client's\n",
    "        local validation set. Then return performance metrics.\"\"\"\n",
    "\n",
    "        self.set_parameters(parameters)\n",
    "        loss, accuracy, precision, recall, f1 = test(\n",
    "            self.model, self.valloader\n",
    "        )  # <-------------------------- calls the `test` function, just what we did in the centralised setting (but this time using the client's local validation set)\n",
    "        # send statistics back to the server\n",
    "        return float(loss), len(self.valloader),{\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1_score\": f1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bneSbJLUDnoZ"
   },
   "outputs": [],
   "source": [
    "def get_evaluate_fn(testloader):\n",
    "    \"\"\"This is a function that returns a function. The returned\n",
    "    function (i.e. `evaluate_fn`) will be executed by the strategy\n",
    "    at the end of each round to evaluate the stat of the global\n",
    "    model.\"\"\"\n",
    "\n",
    "    def evaluate_fn(server_round: int, parameters, config):\n",
    "        \"\"\"This function is executed by the strategy it will instantiate\n",
    "        a model and replace its parameters with those from the global model.\n",
    "        The, the model will be evaluate on the test set (recall this is the\n",
    "        whole test set).\"\"\"\n",
    "\n",
    "        model = Net(num_classes)\n",
    "\n",
    "        # Determine device\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)  # send model to device\n",
    "\n",
    "        # set parameters to the model\n",
    "        params_dict = zip(model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        # call test\n",
    "        loss, accuracy, precision, recall, f1 = test(\n",
    "            model, testloader\n",
    "        )  # <-------------------------- calls the `test` function, just what we did in the centralised setting\n",
    "        return loss, {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1_score\": f1\n",
    "}\n",
    "    return evaluate_fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmY3vOY7D3ZM"
   },
   "outputs": [],
   "source": [
    "def generate_client_fn(trainloaders, valloaders):\n",
    "    def client_fn(cid: str):\n",
    "        \"\"\"Returns a FlowerClient containing the cid-th data partition\"\"\"\n",
    "\n",
    "        return FlowerClient(\n",
    "            trainloader=trainloaders[int(cid)], valloader=valloaders[int(cid)]\n",
    "        ).to_client()\n",
    "\n",
    "    return client_fn\n",
    "\n",
    "\n",
    "client_fn_callback = generate_client_fn(trainloaders, valloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjBxdlQ2D7mb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from flwr.server.strategy import DifferentialPrivacyClientSideFixedClipping\n",
    "from flwr.server.strategy import DifferentialPrivacyServerSideFixedClipping\n",
    "\n",
    "from flwr.server.strategy import DifferentialPrivacyClientSideAdaptiveClipping\n",
    "from flwr.server.strategy import DifferentialPrivacyServerSideAdaptiveClipping\n",
    "\n",
    "from flwr.client.mod import fixedclipping_mod, adaptiveclipping_mod\n",
    "\n",
    "\n",
    "######################### Now we can define the strategy ################################################\n",
    "\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    #fraction_fit=0.1,  # let's sample 10% of the client each round to do local training\n",
    "    #fraction_evaluate=0.1,  # after each round, let's sample 20% of the clients to asses how well the global model is doing\n",
    "    min_fit_clients=NUM_CLIENTS,\n",
    "    min_evaluate_clients=NUM_CLIENTS,\n",
    "    min_available_clients=NUM_CLIENTS,\n",
    "    evaluate_fn=get_evaluate_fn(testloader),\n",
    "    # initial_parameters=parameters\n",
    ") \n",
    "\n",
    "######################### Other strategies ################################################################\n",
    "\n",
    "med_strategy = fl.server.strategy.FedMedian(\n",
    "            min_available_clients=NUM_CLIENTS,\n",
    "            min_fit_clients = NUM_CLIENTS,\n",
    "            min_evaluate_clients = NUM_CLIENTS,\n",
    "            evaluate_fn=get_evaluate_fn(testloader),\n",
    "        )  \n",
    "\n",
    "prox_strategy = fl.server.strategy.FedProx( \n",
    "        # fit_metrics_aggregation_fn=utils.weighted_average,\n",
    "        evaluate_fn=get_evaluate_fn(testloader),\n",
    "        min_fit_clients= NUM_CLIENTS,\n",
    "        min_evaluate_clients= NUM_CLIENTS,\n",
    "        min_available_clients=NUM_CLIENTS,\n",
    "        proximal_mu =1.0 #float!\n",
    "    )\n",
    "\n",
    "# yogi_strategy = fl.server.strategy.FedYogi(\n",
    "#         evaluate_fn=get_evaluate_fn(testloader),\n",
    "#         min_fit_clients=NUM_CLIENTS,\n",
    "#         min_evaluate_clients=NUM_CLIENTS,\n",
    "#         min_available_clients=NUM_CLIENTS,\n",
    "#         initial_parameters=initial_parameters #FedOpt strategies needs initial paramets\n",
    "#     )\n",
    "\n",
    "\n",
    "######################### Add differential privacy ################################################\n",
    "\n",
    "# dp_strategy = DifferentialPrivacyClientSideFixedClipping(\n",
    "#         strategy,\n",
    "#         noise_multiplier=0.2,\n",
    "#         clipping_norm=10,\n",
    "#         num_sampled_clients=NUM_CLIENTS,\n",
    "#     )\n",
    "\n",
    "# #Add fixedclipping_mod to the client-side mods\n",
    "# app = fl.client.ClientApp(\n",
    "#         client_fn=client_fn_callback,\n",
    "#         mods=[\n",
    "#             fixedclipping_mod,\n",
    "#         ]\n",
    "#)\n",
    "\n",
    "dp_strategy = DifferentialPrivacyServerSideAdaptiveClipping(\n",
    "        strategy,\n",
    "        noise_multiplier=0.1,\n",
    "        num_sampled_clients=NUM_CLIENTS,\n",
    "    )\n",
    "\n",
    "# FIND THE RIGHT SETTINGS FOR MAXSPEED, set gpu to zero if you have none\n",
    "client_resources = {\"num_cpus\": 7, \"num_gpus\": 0.5} \n",
    "\n",
    "\n",
    "history = fl.simulation.start_simulation(\n",
    "    client_fn=client_fn_callback,  # a callback to construct a client\n",
    "    num_clients=NUM_CLIENTS,  \n",
    "    config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS),  \n",
    "    strategy=strategy,  # the strategy that will orchestrate the whole FL pipeline\n",
    "    client_resources=client_resources\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot & save csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEW PLOT & SAVE CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strokeorginal = 'StrokeOrignal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract centralized metrics from the history object\n",
    "metrics_dict = history.metrics_centralized\n",
    "\n",
    "# Get the list of rounds from any available metric\n",
    "rounds = [data[0] for data in next(iter(metrics_dict.values()))]\n",
    "\n",
    "# Prepare metric data for CSV\n",
    "metric_names = list(metrics_dict.keys())\n",
    "metric_data_per_round = {metric: [100.0 * val for _, val in metrics_dict[metric]] for metric in metric_names}\n",
    "\n",
    "# Directory to save metrics and plots\n",
    "save_directory = \"/gpfs/home3/rbumbuc1/FedLearning/FedDeepInsight/flower/TABULAR/metrics\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Construct base filename\n",
    "base_filename = f\"DeepInsightPCAv2_{strokeorginal}_{NUM_CLIENTS}C_{NUM_ROUNDS}R_{EPOCHS}E_FedAvg\"\n",
    "csv_filepath = os.path.join(save_directory, f\"{base_filename}.csv\")\n",
    "plot_filepath = os.path.join(save_directory, base_filename + \".png\")\n",
    "\n",
    "# Save all metrics to CSV\n",
    "with open(csv_filepath, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Round'] + [m.capitalize() for m in metric_names])  # Header row\n",
    "    for i, r in enumerate(rounds):\n",
    "        row = [r] + [metric_data_per_round[m][i] for m in metric_names]\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Metrics saved to {csv_filepath}\")\n",
    "\n",
    "# # Plot Accuracy\n",
    "# if \"accuracy\" in metric_data_per_round:\n",
    "#     acc = metric_data_per_round[\"accuracy\"]\n",
    "#     plt.plot(rounds, acc, label=\"Accuracy\")\n",
    "#     plt.grid(True)\n",
    "#     plt.ylabel(\"Accuracy (%)\")\n",
    "#     plt.xlabel(\"Round\")\n",
    "#     plt.title(f\"{DATASET2} Accuracy Over {NUM_ROUNDS} Rounds ({NUM_CLIENTS} clients)\")\n",
    "#     plt.legend()\n",
    "#     plt.savefig(plot_filepath)\n",
    "#     plt.show()\n",
    "#     print(f\"Plot saved to {plot_filepath}\")\n",
    "# else:\n",
    "#     print(\"Accuracy metric not found in history.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################OLD SAVE & PLOT###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZC4YMlQ1rKHH",
    "outputId": "e015183f-4286-49f9-99b0-4f1d5e92b194"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"{history.metrics_centralized = }\")\n",
    "\n",
    "global_accuracy_centralised = history.metrics_centralized[\"accuracy\"]\n",
    "round = [data[0] for data in global_accuracy_centralised]\n",
    "acc = [100.0 * data[1] for data in global_accuracy_centralised]\n",
    "plt.plot(round, acc)\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"Accuracy DeepInsight (%)\")\n",
    "plt.xlabel(\"Round\")\n",
    "plt.title(f\"{DATASET2} Accuracy Over {NUM_ROUNDS} Rounds {NUM_CLIENTS} clients\")\n",
    "# Specify the directory where you want to save the plot\n",
    "save_directory = \"/gpfs/home4/aduah/flower/metrics\"\n",
    " \n",
    "# Ensure the directory exists, create it if it does not\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Construct the full path for the file CANCER\n",
    "save_path = os.path.join(save_directory, f\"DeepInsightPCA_{DATASET2}_{NUM_CLIENTS}C_{NUM_ROUNDS}R_{EPOCHS}E_FedAvgDP01\")\n",
    "\n",
    "# save_path = os.path.join(save_directory, f\"DeepInsight_{DATASET2}_{NUM_CLIENTS}C_{NUM_ROUNDS}R_{EPOCHS}E_FedAvgDP_001\")\n",
    "\n",
    "\n",
    "# Save the plot to the specified directory\n",
    "plt.savefig(save_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "global_accuracy_centralised = history.metrics_centralized[\"accuracy\"]\n",
    "round = [data[0] for data in global_accuracy_centralised]\n",
    "acc = [100.0 * data[1] for data in global_accuracy_centralised]\n",
    "\n",
    "# Define the save directory\n",
    "save_directory = \"/gpfs/home4/aduah/flower/metrics\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "csv_filename = f\"DeepInsightPCA_{DATASET2}_{NUM_CLIENTS}C_{NUM_ROUNDS}R_{EPOCHS}E_FedAvgDP01.csv\"\n",
    "\n",
    "# Define the CSV filename\n",
    "# csv_filename = f\"DeepInsight_{DATASET2}_{NUM_CLIENTS}C_{NUM_ROUNDS}R_{EPOCHS}E_FedAvgDP_001.csv\"\n",
    "\n",
    "# Create the full path for the CSV file\n",
    "csv_filepath = os.path.join(save_directory, csv_filename)\n",
    "\n",
    "# Save the data to the CSV file\n",
    "with open(csv_filepath, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Round', 'Accuracy'])\n",
    "    for r, a in zip(round, acc):\n",
    "        writer.writerow([r, a])\n",
    "\n",
    "print(f\"Data saved to {csv_filepath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdFjdrGeDHHu"
   },
   "source": [
    "-----------------------------\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saves it per round advisable to make a folder and save the rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from typing import List, Union, Optional, Tuple, Dict\n",
    "import flwr as fl\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.common import FitRes, Parameters, Scalar\n",
    "\n",
    "net = Net(num_classes=num_classes).to(device)\n",
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate model weights using weighted average and store checkpoint\"\"\"\n",
    "\n",
    "        # Call aggregate_fit from base class (FedAvg) to aggregate parameters and metrics\n",
    "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(server_round, results, failures)\n",
    "\n",
    "        if aggregated_parameters is not None:\n",
    "            print(f\"Saving round {server_round} aggregated_parameters...\")\n",
    "\n",
    "            # Convert `Parameters` to `List[np.ndarray]`\n",
    "            aggregated_ndarrays: List[np.ndarray] = fl.common.parameters_to_ndarrays(aggregated_parameters)\n",
    "\n",
    "            # Convert `List[np.ndarray]` to PyTorch`state_dict`\n",
    "            params_dict = zip(net.state_dict().keys(), aggregated_ndarrays)\n",
    "            state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "            net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "            # Save the model\n",
    "            save_dir = \"saved_model_STROKE_TSNE\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            # Save the model in the specified directory\n",
    "            save_path = os.path.join(save_dir, f\"model_round_{server_round}.pth\")\n",
    "            torch.save(net.state_dict(), save_path)\n",
    "            print(f\"Model saved at {save_path}\")\n",
    "\n",
    "\n",
    "        return aggregated_parameters, aggregated_metrics\n",
    "strategy = SaveModelStrategy(\n",
    "    #fraction_fit=0.1,  # let's sample 10% of the client each round to do local training\n",
    "    #fraction_evaluate=0.1,  # after each round, let's sample 20% of the clients to asses how well the global model is doing\n",
    "    min_fit_clients=NUM_CLIENTS,\n",
    "    min_evaluate_clients=NUM_CLIENTS,\n",
    "    min_available_clients=NUM_CLIENTS,\n",
    "    evaluate_fn=get_evaluate_fn(testloader),\n",
    ")  # a callback to a function that the strategy can execute to evaluate the state of the global model on a centralised dataset\n",
    "client_resources = {\"num_cpus\": 13, \"num_gpus\": 0.2}\n",
    "\n",
    "history = fl.simulation.start_simulation(\n",
    "    client_fn=client_fn_callback,  # a callback to construct a client\n",
    "    num_clients=NUM_CLIENTS,  # total number of clients in the experiment\n",
    "    config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS),  # let's run for 10 rounds\n",
    "    strategy=strategy,  # the strategy that will orchestrate the whole FL pipeline\n",
    "    client_resources=client_resources\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading saved model\n",
    "# Takes last round in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import os \n",
    "# # list_of_files = [fname for fname in glob.glob(\"./saved_model_rounds/model_round_*\")]\n",
    "# list_of_files = [fname for fname in glob.glob(\"./saved_model_CANCER_5C/model_round_*\")]\n",
    "# latest_round_file = max(list_of_files, key=os.path.getctime)\n",
    "# print(\"Loading pre-trained model from: \", latest_round_file)\n",
    "# state_dict = torch.load(latest_round_file) #load saved model round\n",
    "# net.load_state_dict(state_dict)\n",
    "# net.to(device) #sends model to device\n",
    "# state_dict_ndarrays = [v.cpu().numpy() for v in net.state_dict().values()]\n",
    "# parameters = fl.common.ndarrays_to_parameters(state_dict_ndarrays) # model weights for FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.to('cpu')\n",
    "# model = net.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     y_hat = model(X_train_tensor)\n",
    "# train_predicted = torch.max(y_hat, 1)[1]\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     y_hat = model(X_test_tensor)\n",
    "# test_predicted = torch.max(y_hat, 1)[1]\n",
    "\n",
    "# print(f\"The train accuracy was {accuracy_score(train_predicted, y_train_tensor):.3f}\")\n",
    "# print(f\"The test accuracy was {accuracy_score(test_predicted, y_test_tensor):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through saved model rounds in directory and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "net = Net(num_classes=num_classes).to(device)\n",
    "# Define the directory where the saved models are located\n",
    "model_directory = \"./saved_model_STROKE_TSNE/\"\n",
    "\n",
    "# Get a list of all saved model files\n",
    "list_of_files = [fname for fname in glob.glob(os.path.join(model_directory, \"model_round_*\"))]\n",
    "\n",
    "# Initialize variables to store the best model and its accuracy\n",
    "best_model_file = None\n",
    "best_train_accuracy = 0\n",
    "lst_acc = []\n",
    "\n",
    "# Loop through each saved model file and make predictions\n",
    "for model_file in list_of_files:\n",
    "    print(f\"Loading pre-trained model from: {model_file}\")\n",
    "    \n",
    "    # Load the saved model state\n",
    "    state_dict = torch.load(model_file)\n",
    "    net.load_state_dict(state_dict)\n",
    "    net.to(device)  # Send model to device\n",
    "    \n",
    "    # Convert state_dict to parameters for FL (if needed)\n",
    "    state_dict_ndarrays = [v.cpu().numpy() for v in net.state_dict().values()]\n",
    "    parameters = fl.common.ndarrays_to_parameters(state_dict_ndarrays)  # Model weights for FL\n",
    "    \n",
    "    # Send model to CPU and set to evaluation mode (important)\n",
    "    net.to('cpu')\n",
    "    model = net.eval()\n",
    "    \n",
    "    # Predict on training data\n",
    "    with torch.no_grad():\n",
    "        y_hat_train = model(X_train_tensor)\n",
    "    train_predicted = torch.max(y_hat_train, 1)[1]\n",
    "    \n",
    "    # Predict on test data\n",
    "    with torch.no_grad():\n",
    "        y_hat_test = model(X_test_tensor)\n",
    "    test_predicted = torch.max(y_hat_test, 1)[1]\n",
    "    \n",
    "    # Calculate and print accuracy\n",
    "    train_accuracy = accuracy_score(train_predicted, y_train_tensor)\n",
    "    test_accuracy = accuracy_score(test_predicted, y_test_tensor)\n",
    "    print(f\"Model: {os.path.basename(model_file)} - Train Accuracy: {train_accuracy:.3f}, Test Accuracy: {test_accuracy:.3f}\")\n",
    "\n",
    "    # Extract the round number from the model file name\n",
    "    model_round = os.path.basename(model_file).split('_')[-1]\n",
    "    model_round = int(model_round.split('.')[0])\n",
    "    \n",
    "    # Append the round and train accuracy to the list\n",
    "    lst_acc.append((model_round, train_accuracy))\n",
    "    \n",
    "    # Update the best model if this model has a higher train accuracy\n",
    "    if train_accuracy > best_train_accuracy:\n",
    "        best_train_accuracy = train_accuracy\n",
    "        best_model_file = model_file\n",
    "\n",
    "# Check if a best model was found\n",
    "if best_model_file is not None:\n",
    "    print(f\"Best Model: {os.path.basename(best_model_file)} - Train Accuracy: {best_train_accuracy:.3f}\")\n",
    "else:\n",
    "    print(\"No models were evaluated or no improvement was found over the initial baseline.\")\n",
    "\n",
    "# Calculate and print the mean train accuracy\n",
    "mean_train_accuracy = np.mean([acc for _, acc in lst_acc])\n",
    "print(f\"Mean Train Accuracy: {mean_train_accuracy:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the mean train accuracy\n",
    "df_acc = pd.DataFrame(lst_acc, columns=['Round', 'Accuracy'])\n",
    "df_acc = df_acc.sort_values(by=['Round'])\n",
    "df_acc.to_csv('TSNE_STROKE_syntheticc.csv', index=False)\n",
    "\n",
    "print(df_acc.head())\n",
    "# Plotting the training accuracies per round\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_acc['Round'], df_acc['Accuracy'], marker='o', linestyle='-', color='b', label='Accuracy')\n",
    "plt.title('Training Accuracy per Round Synthetic Dataset')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Train Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "#SAVE PNG\n",
    "save_directory = \"/gpfs/home4/aduah/flower/metrics\"\n",
    " \n",
    "# Ensure the directory exists, create it if it does not\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Construct the full path for the file \n",
    "save_path = os.path.join(save_directory, f\"TSNE_STROKE_synthetic\")\n",
    "\n",
    "# save_path = os.path.join(save_directory, f\"DeepInsight_{DATASET2}_{NUM_CLIENTS}C_{NUM_ROUNDS}R_{EPOCHS}E_FedAvgDP_001\")\n",
    "\n",
    "\n",
    "# Save the plot to the specified directory\n",
    "plt.savefig(save_path)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
